{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7MkbVYn8JZqB2x21m69T+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darkdew/Web-scrapper-jsload-dynamic/blob/main/Australia_Scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YFty3ypNXZO"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}\n",
        "url = 'https://www.medhelp.org/forums/STDs/show/116?page=1'\n",
        "\n",
        "r = requests.get(url, headers= headers)\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "page_count = int(16920/6)\n",
        "page_count\n",
        "full_list=[]\n",
        "for page_cnt in range(1,page_count):\n",
        "        \n",
        "        #generic page url\n",
        "        main_url= 'https://www.medhelp.org/forums/STDs/show/116?page='\n",
        "\n",
        "        #concatenating page number upto 2820\n",
        "        current_url = main_url+ str(page_cnt)\n",
        "        print(current_url)\n",
        "        \n",
        "        #runnign soup on page=n\n",
        "        r0 = requests.get(current_url, headers= headers)\n",
        "        soup0 = BeautifulSoup(r0.text, 'html.parser')\n",
        "        \n",
        "        #counting number of available threads on page=n\n",
        "        threads = soup0.find_all('div',{'class':'username'})\n",
        "        number_of_threads= len(threads)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # r_current_url = requests.get(current_url, headers= headers)\n",
        "        # current_url_soup = BeautifulSoup(r_current_url.text, 'html.parser')\n",
        "        # current_url_thread_name = current_url_soup.find_all('div',{'class':'avt avt-sm'})\n",
        "              \n",
        "        for count in range(number_of_threads):\n",
        "            temp_list=[]\n",
        "            # Extracting thread title\n",
        "            thread_title= soup0.find_all('div',{'class':'subj_header'})[count].find_all('a')[1].text\n",
        "\n",
        "            #Extracting Username\n",
        "            username= soup0.find_all('div',{'class':'username'})[count].find('a').text\n",
        "            print(username)\n",
        "            #Extracting_profile_url\n",
        "            profile_extension = soup0.find_all('div',{'class':'username'})[count].find('a')['href']\n",
        "            profile_url= 'https://www.medhelp.org' + profile_extension\n",
        "\n",
        "            r1 = requests.get(profile_url, headers= headers)\n",
        "            soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
        "\n",
        "            print(profile_url)\n",
        "            #profile_details\n",
        "            profile_details= soup1.find_all('div',{'class':'section'})\n",
        "            breaking= str(profile_details[0]).split('</span>')\n",
        "\n",
        "            #AboutMe\n",
        "            try:\n",
        "              AboutMe= breaking[1].replace('<span>','').replace(',','')\n",
        "\n",
        "            except Exception as e:\n",
        "              AboutMe= 'NA'\n",
        "            #print(AboutMe)\n",
        "\n",
        "            #membership Date\n",
        "            try:\n",
        "              Membership_date= breaking[2].replace('\\n','').replace('</div>','').replace(',','')\n",
        "            #print(Membership_date)\n",
        "            except Exception as e:\n",
        "              Membership_date=breaking[0]\n",
        "            \n",
        "            #part_of_communities\n",
        "            try:\n",
        "              communities= soup1.find('div',{'id':'my_comm'}).find_all('a')\n",
        "              community_list=[]\n",
        "          \n",
        "              for i in range(2,55,3):\n",
        "                try:\n",
        "                  community= communities[i].text\n",
        "                except Exception as e:\n",
        "                  break\n",
        "                community_list.append(community)\n",
        "          \n",
        "            except Exception as e:\n",
        "              community_list='NA'\n",
        "            print(community_list)\n",
        "\n",
        "            temp_list=[thread_title, username, profile_url,AboutMe,Membership_date,community_list]\n",
        "            full_list.append(temp_list)\n",
        "  \n",
        "df = pd.DataFrame(full_list,columns=(\"Thread_Title\",\"Username\",\"Profile_url\",\"AboutMe\",\"Membership_date\",\"is part of Communities\"))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Data/master_df_aus1.csv', index= False)\n",
        "\n",
        "df.head()\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/Colab Data/master_df_aus_2.csv', index=False)\n",
        "import pandas as pd\n",
        "full_final_list=[]\n",
        "for index, row in df.iterrows():\n",
        "    temp_list1 =[]\n",
        "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}\n",
        "    \n",
        "    profile_url= row['Profile_url']\n",
        "    print(profile_url)\n",
        "\n",
        "    #crawling through profile page\n",
        "    r2=wd.get(profile_url)\n",
        "    r3=wd.page_source#)  # results\n",
        "    soup2 = BeautifulSoup(r3, 'html.parser')\n",
        "\n",
        "    #See-All posts url\n",
        "    all_posts_url_value= soup2.find('div',{'id': 'mw_subtitle_2'})\n",
        "    try: \n",
        "      posts_url= all_posts_url_value.find('a')['href']\n",
        "      see_all_posts_url= 'https://www.medhelp.org'+posts_url\n",
        "      print(see_all_posts_url)\n",
        "    \n",
        "    except:\n",
        "      continue\n",
        "    #crawling on all posts page\n",
        "    r4=wd.get(see_all_posts_url)\n",
        "    r5=wd.page_source#)  # results\n",
        "    soup3 = BeautifulSoup(r5, 'html.parser')\n",
        "\n",
        "    #counting posts on page\n",
        "    response_link= soup3.find_all('div',{'class':'subj_user os_12'})\n",
        "\n",
        "    #looping over the available posts\n",
        "    for j in range(len(response_link)):\n",
        "      #capturing response_url\n",
        "      response_link_url= response_link[j].find('a')['href']\n",
        "      response_link_url_final= 'https://www.medhelp.org'+response_link_url\n",
        "      print(response_link_url_final)\n",
        "      \n",
        "      #crawling on specific_response_url\n",
        "      r6=wd.get(response_link_url_final)\n",
        "      r7=wd.page_source#)  # results\n",
        "\n",
        "      soup4 = BeautifulSoup(r7, 'html.parser')\n",
        "\n",
        "      ##\n",
        "      resp_locn= response_link_url_final.split('post_')[1]\n",
        "      div_class= 'mh_vit_resp_ctn post_ctn_'+ resp_locn\n",
        "\n",
        "      #response_text_final\n",
        "      \n",
        "      #len(response_text)\n",
        "      try:\n",
        "        response_text= soup4.find_all('div',{'class':div_class})\n",
        "        #print(response_text)\n",
        "        response_text_final= response_text[0].find('div',{'class':'resp_body '}).text\n",
        "      except Exception as e:\n",
        "        response_text_final= soup4.find_all('div',{'id':'subject_msg'})[0].text\n",
        "      ##response_time_final\n",
        "      try:\n",
        "        response_time_final= response_text[0].find('time',{'class':'mh_timestamp'}).text\n",
        "\n",
        "      except Exception as e:\n",
        "        response_time_final=soup4.find_all('div',{'class':'username'})[0].find('time')[\"datetime\"]\n",
        "      temp_list1= [row['Thread_Title'], row['Username'], row['Profile_url'], row['AboutMe'], row['Membership_date'], row['is part of Communities'], response_text_final, response_time_final]\n",
        "      full_final_list.append(temp_list1)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_final = pd.DataFrame(full_final_list,columns=(\"Thread_Title\",\"Username\",\"Profile_url\",\"AboutMe\",\"Membership_date\",\"is part of Communities\",\"Response_text\",\"Response_time\"))\n",
        "df_final.to_csv('/content/drive/MyDrive/Colab Data/df_aus_final_final.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}